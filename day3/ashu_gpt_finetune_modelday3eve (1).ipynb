{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN74mAALaYwO",
        "outputId": "3570a59a-0ab5-423e-d699-68a3370810e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.112.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.2.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.2.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# installing openai 0.28 # openai 1.0.0\n",
        "!pip install numpy openai==0.28  tiktoken gradio\n",
        "# load data from sample_data directory\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import tiktoken # for token\n",
        "import gradio as gr\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# locate data of CSV type\n",
        "\n",
        "ashu_data = \"/content/sample_data/hotel_hospitality_data.csv\"\n",
        "ashu_clean_data = []\n",
        "# opening file and check data\n",
        "with open(ashu_data,'r',encoding='utf-8-sig') as f1:\n",
        "  ashu_csv_reader = csv.reader(f1)\n",
        "  for i in ashu_csv_reader:\n",
        "    for j in i:\n",
        "      try:\n",
        "        # replace brackets , double quotes ,\n",
        "        replace_data1 = j.replace('[\"','').replace('\"]','').replace('\\\"','\"')\n",
        "        # load each object data in json context\n",
        "        replace_data1_json = json.loads(replace_data1)\n",
        "        # append data in a list for futher saving\n",
        "        ashu_clean_data.append(replace_data1_json)\n",
        "      except json.JSONDecodeError as e:\n",
        "        print(\"some error found\")\n",
        "\n",
        "# lets printing clean data\n",
        "print(ashu_clean_data[0])\n",
        "\n",
        "# save this data for reuse\n",
        "ashu_data_jsonl = \"/content/sample_data/ashu_hotel_hospitality_data.jsonl\"\n",
        "# using file handling\n",
        "with open(ashu_data_jsonl,'w',encoding='utf-8') as f2:\n",
        "  for items in ashu_clean_data:\n",
        "    f2.write(json.dumps(items) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQUK613RbcKu",
        "outputId": "f03974bf-2bb5-4f1f-95c2-9e8c07232d22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}, {'role': 'user', 'content': 'Where is the gym located?'}, {'role': 'assistant', 'content': \"Our gym is located on the 2nd floor. I'll be happy to show you the way if you need help!\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing data for validataion purpose\n",
        "with open(ashu_data_jsonl,'r',encoding='utf-8') as f3:\n",
        "  ashu_datasets = [ json.loads(i) for i in f3]\n",
        "\n",
        "# total length\n",
        "print(\"length of data dataset is \",len(ashu_datasets))\n",
        "print(\"first example \")\n",
        "for ashud in ashu_datasets[0]['messages']:\n",
        "  print(ashud)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hO0jNThcSKr",
        "outputId": "b0025c33-930b-4fac-e465-00039456cdac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of data dataset is  18\n",
            "first example \n",
            "{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}\n",
            "{'role': 'user', 'content': 'Where is the gym located?'}\n",
            "{'role': 'assistant', 'content': \"Our gym is located on the 2nd floor. I'll be happy to show you the way if you need help!\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#**format validation\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in ashu_datasets:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwuH-i18n_R_",
        "outputId": "e3eabf84-5df9-46d3-c069-23d62137131d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No errors found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ],
      "metadata": {
        "id": "OZJnkROvsmxP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in ashu_datasets:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "#n_too_long = sum(l > 16,385 for l in convo_lens)\n",
        "#print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2LQYcFXuTyX",
        "outputId": "9b26be8c-49e7-4056-9758-184609be4d6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 68, 91\n",
            "mean / median: 76.38888888888889, 74.0\n",
            "p5 / p95: 71.4, 87.5\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 16, 39\n",
            "mean / median: 24.444444444444443, 23.0\n",
            "p5 / p95: 19.0, 35.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#UPloading data to gpt model\n",
        "import openai\n",
        "# loading api keys\n",
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "nR_ZojyFyMKE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# please don't upload the same data\n",
        "# # uploading jsonl data to gpt api server\n",
        "# training_data = \"/content/sample_data/ashu_hotel_hospitality_data.jsonl\"\n",
        "# # creting openai method for uplaod\n",
        "# training_response = openai.File.create(file=open(training_data,'rb'), purpose=\"fine-tune\")\n",
        "# # print response\n",
        "# print(training_response)\n",
        "# # print file id which we are uploading\n",
        "# ashu_file_id = training_response[\"id\"]\n",
        "# print(\"ashu uploaded file id is \",ashu_file_id)"
      ],
      "metadata": {
        "id": "jL3JQNGeyvbF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # dont not perform\n",
        "# # creating fine tuning job\n",
        "# ashu_fm_suffix = \"ashu_hotel_bot\"\n",
        "# # creting fine tune arguments\n",
        "# fineTune_response = openai.FineTuningJob.create(\n",
        "#     training_file=ashu_file_id,\n",
        "#     suffix=ashu_fm_suffix,\n",
        "#     model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# # print id\n",
        "# ashu_job_id=fineTune_response[\"id\"]\n",
        "\n",
        "# print(fineTune_response)\n"
      ],
      "metadata": {
        "id": "dvaQVoXt2Xq9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # list training job events\n",
        "# #list events as fine-tuning progresses\n",
        "# response = openai.FineTuningJob.list_events(id=ashu_job_id, limit=50)\n",
        "\n",
        "# events = response[\"data\"]\n",
        "# events.reverse()\n",
        "\n",
        "# for event in events:\n",
        "#     print(event[\"message\"])"
      ],
      "metadata": {
        "id": "ZMJ1VE6-6Z2E"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can retive existing fine tuned model ids\n",
        "myaccount_models = openai.Model.list()\n",
        "#print(myaccount_models['data'])\n",
        "# listing it using loop\n",
        "for model in myaccount_models['data']:\n",
        "  print(model['id'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz3opo_6wZ-G",
        "outputId": "76b2dc2f-b38f-4a78-8113-e01343c63bc5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dall-e-3\n",
            "gpt-4-1106-preview\n",
            "dall-e-2\n",
            "tts-1-hd-1106\n",
            "tts-1-hd\n",
            "text-embedding-3-large\n",
            "gpt-4-0125-preview\n",
            "babbage-002\n",
            "gpt-4-turbo-preview\n",
            "gpt-4o\n",
            "gpt-4o-2024-05-13\n",
            "text-embedding-3-small\n",
            "tts-1\n",
            "gpt-3.5-turbo\n",
            "whisper-1\n",
            "gpt-4o-2024-08-06\n",
            "text-embedding-ada-002\n",
            "gpt-3.5-turbo-16k\n",
            "davinci-002\n",
            "gpt-4-turbo-2024-04-09\n",
            "tts-1-1106\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-4-turbo\n",
            "gpt-3.5-turbo-1106\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o-mini\n",
            "gpt-4-0613\n",
            "gpt-4\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-walmartbot:9tVvJxZA:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-walmartbot:9tVvJe6E:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-walmartbot:9tVvJ5oT\n",
            "ft:gpt-3.5-turbo-0125:personal:samantha-test:9tW4MOZd\n",
            "ft:gpt-3.5-turbo-0125:personal:samantha-test:9tW4MFcG:ckpt-step-114\n",
            "ft:gpt-3.5-turbo-0125:personal:samantha-test:9tW4MwCX:ckpt-step-57\n",
            "ft:gpt-3.5-turbo-0125:personal:hotel-bot:9teGNfPK:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:hotel-bot:9teGNgSX:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:hotel-bot:9teGN9Gl\n",
            "ft:gpt-3.5-turbo-0125:personal:hotel-bot:9teHDHFw:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:hotel-bot:9teHDG2a:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:hotel-bot:9teHEIQa\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9teHfpnU:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9teHgqf9:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9teHgxTA\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tf25vvY:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDPIC:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDQOW:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDM1s\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tf24xX5:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tf25WXg\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJBnwBd:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJHvgu0:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tJ5jMgn:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tJ5j93I:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tJ5j4qV\n",
            "ft:gpt-3.5-turbo-0125:personal:ps-hotel-bot:9tJ7Du6I:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:ps-hotel-bot:9tJ7EmiE:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:ps-hotel-bot:9tJ7Ef0a\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJHwsIv\n",
            "ft:gpt-3.5-turbo-0125:personal::9tJ7yW3o:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal::9tJ7y0Rm:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal::9tJ7yuLr\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJBngi4:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJBn1K9\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJE5Yl0:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJE6Or6:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJE6IEC\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJHvCpQ:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:sekhar-hotel-bot:9tJQE02x\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJEuQ02:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJEudsN:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJEuzKM\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tJJJwbk:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tJJKsQU:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:mg-hotel-bot:9tJJKZnD\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJJNpyu:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJJOrRZ:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:ashu-hotel-bot:9tJJO9P1\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJK0TRr:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJK0Kiv:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:sal-hotel-bots:9tJK0A4Z\n",
            "ft:gpt-3.5-turbo-0125:personal:sekhar-hotel-bot:9tJQDSNT:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:sekhar-hotel-bot:9tJQDhTy:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:zac-hotel-bot:9tJYflyv:ckpt-step-54\n",
            "ft:gpt-3.5-turbo-0125:personal:zac-hotel-bot:9tJYg1Sf:ckpt-step-72\n",
            "ft:gpt-3.5-turbo-0125:personal:zac-hotel-bot:9tJYga2M\n",
            "ft:gpt-3.5-turbo-0125:personal:rm-hotel-bot:9tJb5xcj:ckpt-step-18\n",
            "ft:gpt-3.5-turbo-0125:personal:rm-hotel-bot:9tJb5aZk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating data for input purpose for testing the fine tuned model\n",
        "ashu_test_data = []\n",
        "system_message = \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "user_input = \"hey i am willing to park my BMW car please suggest some good and safe options\"\n",
        "\n",
        "# creating message format\n",
        "ashu_test_data.append({ \"role\": \"system\" , \"content\" : system_message})\n",
        "# adding user input\n",
        "\n",
        "ashu_test_data.append({ \"role\": \"user\" , \"content\" : user_input})\n",
        "# print user input to test\n",
        "print(ashu_test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl7KohOk9ySz",
        "outputId": "fcaad952-8254-4e0c-fcdc-8d74039872ea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}, {'role': 'user', 'content': 'hey i am willing to park my BMW car please suggest some good and safe options'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling fine model on hotel hospitality data to get response\n",
        "ashu_response = openai.ChatCompletion.create(\n",
        "    model=\"ft:gpt-3.5-turbo-0125:personal:ashu-walmartbot:9tVvJxZA:ckpt-step-54\",\n",
        "    messages=ashu_test_data,\n",
        "    max_tokens=500,\n",
        "    temperature=0\n",
        ")\n",
        "# lets print response details\n",
        "print(ashu_response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLjnv5U6_slC",
        "outputId": "107558e7-b007-440b-f228-1ade2f733ebb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolutely! We have secure parking options available. You can park your BMW in our designated parking area. I'll guide you there.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating GUI internface\n",
        "def ashu_get_response(user_input):\n",
        "  ashu_test_data = []\n",
        "  system_message = \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "  # creating message format\n",
        "  ashu_test_data.append({ \"role\": \"system\" , \"content\" : system_message})\n",
        "  # adding user input\n",
        "\n",
        "  ashu_test_data.append({ \"role\": \"user\" , \"content\" : user_input})\n",
        "\n",
        "  ashu_response = openai.ChatCompletion.create(\n",
        "    model=\"ft:gpt-3.5-turbo-0125:personal:ashu-walmartbot:9tVvJxZA:ckpt-step-54\",\n",
        "    messages=ashu_test_data,\n",
        "    max_tokens=500,\n",
        "    temperature=0\n",
        ")\n",
        "  return ashu_response['choices'][0]['message']['content']\n",
        "\n",
        "# creating gui\n",
        "# Set up the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=ashu_get_response,\n",
        "    inputs=gr.Textbox(label=\"Enter your question:\"),\n",
        "    outputs=gr.Textbox(label=\"Response: \"),\n",
        "    title=\"Chatbot Interface\",\n",
        "    description=\"Ask questions to the friendly hospitality chatbot.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "wY05PzbzFqlo",
        "outputId": "4788b2ba-99a5-4899-94cd-bbcdac352ba1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://50f50f5f9319162ad6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://50f50f5f9319162ad6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}